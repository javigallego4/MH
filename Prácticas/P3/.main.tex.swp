%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Plantilla para libro de texto de matemáticas.
%
% Esta plantilla ha sido desarrollada desde cero, pero utiliza algunas partes
% del código de la plantilla original utilizada en apuntesDGIIM
% (https://github.com/libreim/apuntesDGIIM), basada a su vez en las plantillas
% 'Short Sectioned Assignment' de Frits Wenneker (http://www.howtotex.com),
% 'Plantilla de Trabajo' de Mario Román y 'Plantilla básica de Latex en Español'
% de Andrés Herrera Poyatos (https://github.com/andreshp). También recoge
% ideas de la plantilla 'Multi-Purpose Large Font Title Page' de
% Frits Wenneker y Vel (vel@latextemplates.com).
%
% Licencia:	
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ---------------------------------------------------------------------------
% CONFIGURACIÓN BÁSICA DEL DOCUMENTO
% ---------------------------------------------------------------------------

%\documentclass[11pt, a4paper, twoside]{article} % Usar para imprimir
\documentclass[10pt, a4paper]{article}

\linespread{1.3}            % Espaciado entre líneas.
\setlength\parindent{0pt}   % No indentar el texto por defecto.
\setlength\parskip{7pt}

% ---------------------------------------------------------------------------
% PAQUETES BÁSICOS
% ---------------------------------------------------------------------------

% IDIOMA
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla, es-lcroman, es-noquoting]{babel}
\usepackage[table,xcdraw]{xcolor}
\usepackage{longtable}
\usepackage{subfigure}

% MATEMÁTICAS
\usepackage{amsmath}    % Paquete básico de matemáticas
\usepackage{amsthm}     % Teoremas
\usepackage{mathrsfs}   % Fuente para ciertas letras utilizadas en matemáticas

% FUENTES
\usepackage{newpxtext, newpxmath}   % Fuente similar a Palatino
\usepackage{FiraSans}                 % Fuente sans serif
\usepackage[T1]{fontenc}
\usepackage[italic]{mathastext}     % Utiliza la fuente del documento
                                    % en los entornos matemáticos

% MÁRGENES
\usepackage[margin=2.5cm, top=3cm]{geometry}

% LISTAS
\usepackage{enumitem}       % Mejores listas
\setlist{leftmargin=.5in}   % Especifica la indentación para las listas.

% Listas ordenadas con números romanos (i), (ii), etc.
\newenvironment{nlist}
{\begin{enumerate}
    \renewcommand\labelenumi{(\emph{\roman{enumi})}}}
  {\end{enumerate}}

%  OTROS
\usepackage[hidelinks]{hyperref}   % Enlaces
\usepackage{graphicx}   % Permite incluir gráficos en el documento
\usepackage{relsize}

% LISTINGS
\usepackage{listings}
\usepackage{xcolor}     % Permite definir y utilizar colores
\usepackage{lipsum}
\usepackage{courier}

% Fijar tabla a posición
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\setcounter{MaxMatrixCols}{20}

% Colores para los bloques de código
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour!70!white},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=4
}
\lstset{style=mystyle}

\usepackage{amsmath}
\usepackage{algorithm}


\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

%\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
%\lstset{framextopmargin=50pt,frame=bottomline}
 
% ---------------------------------------------------------------------------
% COMANDOS PERSONALIZADOS
% ---------------------------------------------------------------------------

% \equalto
\newcommand{\verteq}{\rotatebox{90}{$\,=$}}
\newcommand{\equalto}[2]{\underset{\scriptstyle\overset{\mkern4mu\verteq}{#2}}{#1}}


% ---------------------------------------------------------------------------
% COLORES
% ---------------------------------------------------------------------------

\definecolor{50}{HTML}{E0F2F1}
\definecolor{100}{HTML}{B2DFDB}
\definecolor{200}{HTML}{80CBC4}
\definecolor{300}{HTML}{4DB6AC}
\definecolor{400}{HTML}{26A69A}
\definecolor{500}{HTML}{009688}
\definecolor{600}{HTML}{00897B}
\definecolor{700}{HTML}{00796B}
\definecolor{800}{HTML}{00695C}
\definecolor{900}{HTML}{004D40}
\definecolor{ugrColor}{HTML}{c6474b}  % Usado en el título.
\definecolor{ugrColor2}{HTML}{c6474b} % Usado en las secciones.

% ---------------------------------------------------------------------------
% DISEÑO DE PÁGINA
% ---------------------------------------------------------------------------

\usepackage{pagecolor}
\usepackage{afterpage}

% ---------------------------------------------------------------------------
% CABECERA Y PIE DE PÁGINA
% ---------------------------------------------------------------------------

\usepackage{fancyhdr}   % Paquete para cabeceras y pies de página

% Indica que las páginas usarán la configuración de fancyhdr
\pagestyle{fancy}
\fancyhf{}

% Representa la sección de la cabecera
\renewcommand{\sectionmark}[1]{%
\markboth{#1}{}}

% Representa la subsección de la cabecera
\renewcommand{\subsectionmark}[1]{%
\markright{#1}{}}

% Parte derecha de la cabecera
\fancyhead[LE,RO]{\sffamily\textsl{\rightmark} \hspace{1em}  \textcolor{ugrColor2}{\rule[-0.4ex]{0.2ex}{1.2em}} \hspace{1em} \thepage}

% Parte izquierda de la cabecera
\fancyhead[RE,LO]{\sffamily{\leftmark}}

% Elimina la línea de la cabecera
\renewcommand{\headrulewidth}{0pt}

% Controla la altura de la cabecera para que no haya errores
\setlength{\headheight}{14pt}

% ---------------------------------------------------------------------------
% TÍTULOS DE PARTES Y SECCIONES
% ---------------------------------------------------------------------------

\usepackage{titlesec}

% Estilo de los títulos de las partes
\titleformat{\part}[hang]{\Huge\bfseries\sffamily}{\thepart\hspace{20pt}\textcolor{ugrColor}{|}\hspace{20pt}}{0pt}{\Huge\bfseries}
\titlespacing*{\part}{0cm}{-2em}{2em}[0pt]

% Reiniciamos el contador de secciones entre partes (opcional)
\makeatletter
\@addtoreset{section}{part}
\makeatother

% Estilo de los títulos de las secciones, subsecciones y subsubsecciones
\titleformat{\section}
  {\Large\bfseries\sffamily}{\thesection}{1em}{}

\titleformat{\subsection}
  {\Large\sffamily}{\thesubsection}{1em}{}[\vspace{.5em}]

\titleformat{\subsubsection}
  {\sffamily}{\thesubsubsection}{1em}{}

% ---------------------------------------------------------------------------
% ENTORNOS PERSONALIZADOS
% ---------------------------------------------------------------------------

\usepackage{mdframed}

%% DEFINICIONES DE LOS ESTILOS

% Nuevo estilo para definiciones
\newtheoremstyle{definition-style}  % Nombre del estilo
{}                                  % Espacio por encima
{}                                  % Espacio por debajo
{}                                  % Fuente del cuerpo
{}                                  % Identación
{\bf\sffamily}                      % Fuente para la cabecera
{.}                                 % Puntuación tras la cabecera
{.5em}                              % Espacio tras la cabecera
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}  % Especificación de la cabecera

% Nuevo estilo para notas
\newtheoremstyle{remark-style}
{10pt}
{10pt}
{}
{}
{\itshape \sffamily}
{.}
{.5em}
{}

% Nuevo estilo para teoremas y proposiciones
\newtheoremstyle{theorem-style}
{}
{}
{}
{}
{\bfseries \sffamily}
{.}
{.5em}
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}

% Nuevo estilo para teoremas y proposiciones
\newtheoremstyle{theorem2-style}
{}
{}
{}
{}
{\bfseries \sffamily}
{.}
{.5em}
{\thmname{#1}\thmnote{ (#3)}}

% Nuevo estilo para ejemplos
\newtheoremstyle{example-style}
{10pt}
{10pt}
{}
{}
{\bf \sffamily}
{}
{.5em}
{\thmname{#1}\thmnumber{ #2.}\thmnote{ #3.}}

% Nuevo estilo para la demostración

\makeatletter
\renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\itshape\sffamily#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse}
\makeatother

%% ASIGNACIÓN DE LOS ESTILOS

% Teoremas, proposiciones y corolarios
\newtheoremstyle{theorem-style}{}{}{}{}{}{}{ }{}
\theoremstyle{theorem-style}
\newtheorem*{datos}{}
\theoremstyle{theorem-style}
\newtheorem{nth}{Teorema}[section]
\newtheorem{nprop}{Proposición}[section]
\newtheorem{ncor}{Corolario}[section]
\newtheorem{lema}{Lema}[section]
\theoremstyle{theorem2-style}
\newtheorem{demostracion}{\textbf{\emph{Demostración}}}

% Definiciones
\theoremstyle{definition-style}
\newtheorem{ndef}{Definición}[section]

% Notas
\theoremstyle{remark-style}
\newtheorem*{nota}{Nota}

% Ejemplos
\theoremstyle{example-style}
\newtheorem{ejemplo}{Ejemplo}[section]

% Ejercicios y solución
\theoremstyle{definition-style}
\newtheorem{ejer}{Ejercicio}[section]

\theoremstyle{remark-style}
\newtheorem*{sol}{Solución}

\theoremstyle{remark-style}
\newtheorem*{dem}{Demostración}

%% MARCOS DE LOS ESTILOS

% Configuración general de mdframe, los estilos de los teoremas, etc
\mdfsetup{
  skipabove=1em,
  skipbelow=1em,
  innertopmargin=1em,
  innerbottommargin=1em,
  splittopskip=2\topsep,
}

% Definimos los marcos de los estilos


\mdfdefinestyle{datos-frame}{
	linewidth=2pt, %
	linecolor= ugrColor, %
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0em, %
	innerleftmargin=1em, %
	innerrightmargin=1em,
	rightmargin=0em, %
}%
\mdfdefinestyle{nth-frame}{
	linewidth=2pt, %
	linecolor= 500, %
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0em, %
	innerleftmargin=1em, %
  innerrightmargin=1em,
	rightmargin=0em, %
}%

\mdfdefinestyle{nprop-frame}{
	linewidth=2pt, %
	linecolor= 300, %
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0pt, %
	innerleftmargin=1em, %
	innerrightmargin=1em,
	rightmargin=0pt, %
}%

\mdfdefinestyle{dem_mia-frame}{
	linewidth=2pt, %
	linecolor= ugrColor, %
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0pt, %
	innerleftmargin=1em, %
	innerrightmargin=1em,
	rightmargin=0pt, %
}%

\mdfdefinestyle{ndef-frame}{
	linewidth=2pt, %
	linecolor= 500, %
	backgroundcolor= 50,
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0pt, %
	innerleftmargin=1em, %
	innerrightmargin=1em,
	rightmargin=0pt, %
}%

\mdfdefinestyle{ejer-frame}{
	linewidth=2pt, %
	linecolor= ugrColor, %
	backgroundcolor= 50,
	topline=false, %
	bottomline=false, %
	rightline=false,%
	leftmargin=0pt, %
	innerleftmargin=1em, %
	innerrightmargin=1em,
	rightmargin=0pt, %
}%

\mdfdefinestyle{ejemplo-frame}{
	linewidth=0pt, %
	linecolor= 300, %
	leftline=false, %
	rightline=false, %
	leftmargin=0pt, %
	innerleftmargin=1.3em, %
	innerrightmargin=1em,
	rightmargin=0pt, %
	innertopmargin=0em,%
	innerbottommargin=0em, %
	splittopskip=\topskip, %
}%

% Asignamos los marcos a los estilos
\surroundwithmdframed[style=nth-frame]{nth}
\surroundwithmdframed[style=datos-frame]{datos}
\surroundwithmdframed[style=nprop-frame]{nprop}
\surroundwithmdframed[style=nprop-frame]{ncor}
\surroundwithmdframed[style=ndef-frame]{ndef}
\surroundwithmdframed[style=ejer-frame]{ejer}
\surroundwithmdframed[style=ejemplo-frame]{ejemplo}
\surroundwithmdframed[style=ejemplo-frame]{sol}
\surroundwithmdframed[style=dem_mia-frame]{demostracion}

% ---------------------------------------------------------------------------
% CONFIGURACIÓN DE LA PORTADA
% ---------------------------------------------------------------------------

\newcommand{\asignatura}{Metaheurísticas}
\newcommand{\universidad}{Universidad de Granada}

% ---------------------------------------------------------------------------
% FOTO DE LA PORTADA
% ---------------------------------------------------------------------------




\title{\vspace{3cm}\textcolor{ugrColor}{\textbf{{{{\Huge Aprendizaje de Pesos en Características}}}}}}

\usepackage{tcolorbox}
\newtcolorbox{example}[2][]
{colback=ugrColor!25!white,colframe=ugrColor!90!white,
fonttitle=\bfseries, title=Enunciado~\thetcbcounter: #2,#1}

\usepackage{titling}
\include{titlepage}
\usepackage{wallpaper}
\date{12 de mayo 2022}

\usepackage{graphicx}
\begin{document}
\ThisULCornerWallPaper{1}{ugrA4.pdf}
\maketitle

\begin{center}
\large
\vspace{2cm}
\textbf{\emph{\Large Metaheurísticas}}\\
\emph{\large Gallego Menor, Francisco Javier}\\
\emph{\large javigallego@correo.ugr.es}\\
\emph{\large 74745747W}\\

\end{center}
              
\newpage


\tableofcontents
\newpage

\section{Introducción}
El problema de clasificación consiste en, dado un conjunto $A=\{(a,b) \ : a \in R^n, b \ es \ una \ clase\}$ de datos ya clasificados, obtener un sistema que permita clasificar un objeto nuevo de forma automática.\\

 Un ejemplo de clasificador, y el que utilizaremos en esta práctica,es el $k-NN$ , $k$ vecinos más cercanos. Este toma la clase que más se repita entre los $u_i\in A$ tales que su distancia al nuevo elemento $u$ sea mínima. En nuestro caso, en una versión sencilla del problema, consideraremos el clasificador $1-NN$. \\

Consideraremos como distancias la distancia trivial si las características son discretas (esto es, la distancia será 1 si las características son diferentes, y 0 si son iguales. La denotamos como $d_n$), y la distancia euclídea para características que sean continuas. Además, cada característica tendrá un peso asociado, por lo que dado un vector de pesos $w$, la distancia entre dos vectores $u$ y $v$ será de la forma:
\[
d(u,v) = \sqrt {\sum_i w_i(u_i- v_i)^2 + \sum_j w_j d_n(u_j,v_j)}
\]

 El aprendizaje de pesos en características consiste  en hallar un vector de pesos que maximice la siguiente función:
\[
F(w) = \alpha T_{clas}(w) + (1-\alpha)T_{red}(w)
\]
Donde
\begin{itemize}
\item $T_{clas}$ es la función que indica cómo de bueno es nuestro clasificador, es decir, cuántos casos ha clasificado correctamente si entrenamos el clasificador usando el resto de datos ,la técnica \emph{k-fold cross validation}, y dejando un elemento fuera (leave one out).
	\item $T_{red}$ que es la función que nos indica cuántas características de un dato tienen un peso menor que un valor establecido, en nuestro caso $0.2$.
\end{itemize}


\section{Descripción de la aplicación de los algoritmos}

\subsection{Esquemas de representación}
\textcolor{ugrColor}{Antes de comenzar, comentar brevemente que aquellas secciones que no añaden nada nuevo (aquellas cogidas de la P1) las marco en rojo}. En nuestro problema, los datos de entrada poseen los elementos que siguen: 
\begin{itemize}	
	\item \textbf{Clase del elemento (target)}: que es la categoría a la que corresponde el mismo, dependiendo de cada dataset
	\item \textbf{Ejemplo}: que es un par que tiene un vector de features y una clase (target).
	\item \textbf{Dataset}: que contendrá una lista de ejemplos
	\item \textbf{Vector de características}: vector de valores reales que trataremos de normalizar al intervalo $[0,1]$ para trabajar con ellos.	

\end{itemize}
A partir de los elementos previamente descritos, vamos a obtener nuestra solución al problema. Esta consistirá en un vector de pesos, cuyos valores reales se encontrarán también en el intervalo $[0,1]$.

\subsection{Operadores comunes}
En esta sección, procederemos a describir aquellas funcionalidades que sean comunes para todos los algoritmos. En nuestro caso, solo es la que sigue: 

\subsubsection{Generación Soluciones Aleatorias}

Para la generación de las soluciones aleatorias usamos \textbf{np.random.uniform}, el cual implica que cualquier valor dentro del intervalo dado tiene la misma probabilidad de ser extraído por uniforme. En nuestro caso el [0,1). El pseudocódigo es el siguiente: \\

\begin{algorithmic}[1]
\Procedure{Crear Solución Aleatoria}{}:
\State vector de pesos $\gets$ np.random.uniform(extremo inferior intervalo, extremo superior, longitud del vector)
\EndProcedure
\end{algorithmic}

VERIFICAR ESTO. NO ES COMUN A TODOS PORQUE HAY ALGUNOS QUE NO LOS USAN .

\subsubsection{\textcolor{ugrColor}{Mutación}}
Este operador es común para todos los algoritmos de la práctica. Al mutar, sumaremos a la componente $j-esima$ un valor aleatorio y truncaremos al intervalo $[0,1]$ si el nuevo valor se nos escapara del intervalo:\\
\begin{algorithmic}[1]
\Procedure{mute}{w,sigma,j}
\State $w(j) \gets w(j) + random(0,1, scale = sigma)$
\State $w(j) \gets Normalize(w(j))$
\State \textbf{return} w
\EndProcedure
\end{algorithmic}

\subsection{Función objetivo}
En nuestro caso, se nos indica que tomemos como $alpha$ el valor $0.5$, así que en realidad lo que estamos haciendo es:
\[
F = 0.5(T_{clas} + T_{red})
\]
Hemos modificado la forma de calcular  $T_{class}$ con respecto a su versión de la P1, pues era el cuello de botella en la ejecución. La calcularemos de la siguiente manera ahora:\\

\begin{algorithmic}[1]
\Procedure{T-class}{w, features, targets}
\State dataw $\gets$ features * w
\State classifier $\gets$ clasificador 1-NN de Sklearn
\State Entrenamos el modelo
\State ind\_near $\gets$ indice vecino mas cercano
\State tasa\_class $\gets$ media (targets[ind\_near] == targets)
\State \textbf{return} tasa\_class / 100

\EndProcedure
\end{algorithmic}

Y calcularemos $T_{red}$ así:\\
\begin{algorithmic}[1]
\Procedure{T-red}{weight}
\State \textbf{return} (Nº Pesos < 0.2) / Nº Pesos Totales
\EndProcedure
\end{algorithmic}

\section{Descripción de los algoritmos considerados}

\subsection{\textcolor{ugrColor}{Algoritmo Greedy Relief}}
El algoritmo de Greedy Relief se encarga de recorrer todo el conjunto de datos, muestra por muestra. En cada uno de ellos, dependiendo del amigo y enemigo más cercanos, hace una modificación del vector de pesos. Entonces, lo que haremos será: por cada una de las muestras, obtener un conjunto de datos amigos (que pertenecen a su misma clase) y uno de enemigos (no pertenecen a su misma clase). Posteriormente, obtendremos el vecino más cercano de cada uno de ellos. \\

\begin{algorithm}
\caption{Greedy Relief}\label{euclid}
\begin{algorithmic}[1]
\Procedure{greedyRelief}{features, targets}
\State $w \gets [0, ...., 0]$
\State distances $\gets$ matriz cuadrada de distancias (euclídeas)
\BState \textbf{\emph{loop}:} para cada muestra del conjunto de entrenamiento

\State en\_indices $\gets$ índices de ejemplos con distinto target a la muestra

\State fr\_indices $\gets$ índices de ejemplos con igual target a la muestra

\State friends $\gets$ features[fr\_indices]
\State enemies $\gets$ features[en\_indices]
\State
\State closestFriend $\gets$ Amigo mas cercano
\State closestEnemy $\gets$ Enemigo mas cercano
\State
\State w $\gets$ w + |features(i) - closestEnemy| - |features(i)-closestFriend|
\BState \textbf{\emph{endloop}}
\State w $\gets$ Truncamos valores negativos a 0
\State w $\gets$ Normalizamos w
\State \textbf{return} w
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{\textcolor{ugrColor}{Búsqueda local}}
Para este algoritmo, debemos definir el algoritmo que hemos usado para obtener una mutación de un ejemplo. Al mutar, sumaremos a la componente $j-esima$ un valor aleatorio y truncaremos al intervalo $[0,1]$ si el nuevo valor se nos escapara del intervalo:
\begin{algorithmic}[1]
\Procedure{mute}{w,sigma,j}
\State $w(j) \gets w(j) + random(0,1)$
\State $w(j) \gets Normalize(w(j))$
\State \textbf{return} w
\EndProcedure
\end{algorithmic}
En el procedimiento de cálculo de pesos mediante la búsqueda local intervendrá la función de evaluación, que será notada por $f(w)$.

Así, el procedimiento general para la generación de pesos para la búsqueda local sería:


\begin{algorithm}
\caption{Local Search}\label{euclid}
\begin{algorithmic}[1]
\Procedure{localSearch}{initialWeight,features,targets}
\State weight $\gets$ valor inicial para los pesos
\State bestF $\gets$ valor inicial para la función objetivo
\State index $\gets$ Ordenación de forma aleatoria de los índices
\State improve $\gets$ False
\BState \textbf{\emph{while}}  (menos evaluaciones que 15000) and (Haya mejora antes de generar 20*n vecinos) 

\State w $\gets$ vector de pesos mutando componente j-esima

\State newF $\gets$ Valor funcion objetivo para w
\If {newF $>$ bestF}

bestF $\gets$ newF

weight $\gets$ w

notMuted $\gets$ 0

improve $\gets$ True

\Else

notMuted $\gets$ notMuted + 1

\EndIf

\State evaluaciones $\gets$ evaluaciones +1

\If {(Hay mejora) or (Todos los componentes han sido ya mutados)}

improve $\gets$ False

index $\gets$ Ordenación de forma aleatoria de los índices

\EndIf

\BState \emph{endloop}
\State \textbf{return} weight
\EndProcedure
\end{algorithmic}
\end{algorithm}

\pagebreak
\subsection{Búsqueda Multiarranque Básica (BMB)}

Es el algoritmo multiarranque más básico que podemos considerar. En él,  las soluciones iniciales se generan al azar, y la etapa de búsqueda se realiza
mediante el algoritmo de Búsqueda Local. Siguiendo el pseudocódigo de los seminarios y teoría, se ha implementado una versión concisa aprovechando la búsqueda local implementada en las sesiones anteriores.
\vspace{0.2cm}

\begin{algorithm}
\caption{BMB}\label{euclid}
\begin{algorithmic}[1]
\Procedure{BMB}{data,classes, trainIndex, testIndex}
\State ini\_weights $\gets$ generamos la solución inicial
\State bestF $\gets$ f(ini\_weights)
\State
\BState \textbf{\emph{for i in}}
(range(15)):
\State w $\gets$ localSearch(ini\_weights, data, classes) \hspace{3cm}(15000 / 15 evaluaciones)

\State currentF $\gets$ f(w)
\State
\If {currentF > bestF}
\State weights $\gets$ np.copy(w)
\State bestF $\gets$ currentF
\EndIf
\State
\State ini\_weights $\gets$ generamos nueva solución aleatoria inicial

\BState \emph{endloop}
\State Entrenamos el modelo, predecimos y calculamos las tasas
\State \textbf{return} tasa clasificación, tasa reducción
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Enfriamiento Simulado}

Este algoritmo es bastante más complejo que el anterior. Empezamos comentando algunas particularidades. Para el cálculo de la temperatura inicial se sigue el siguiente esquema: $T_0 = 0.3 * f(inicial) / -ln(0.3)$. Donde $f(x)$ es nuestra función fitness. La temperatura inicial se calcula como el mínimo entre $10^{-3}$ y $T_0$, para que nunca sea la temperatura final mayor que la inicial.

Por otro lado, existen varios criterios de parada. Para el bucle externo el criterio de parada es el número de evaluaciones de la función fitness y la temperatura actual. Mientras que en el bucle interno es el número de vecinos generados y el número de soluciones aceptadas (entre cada enfriamiento).

Finalmente el criterio de enfriamiento es Cauchy modificado donde tenemos que $T_{i+1} = T_i / (1 + \beta * T_i)$, para $\beta = (T_0 - T_f) / (M * T_0 * T_f)$. M es el número de enfriamientos a realizar, es decir el número máximo de evaluaciones a la función fitness, entre el número de vecinos que se genera en cada bucle interno. 

\begin{algorithm}
\caption{ES}\label{euclid}
\begin{algorithmic}[1]
\Procedure{ES}{data,classes, trainIndex, testIndex}
\State weights $\gets$ generamos la solución inicial
\State bestF, currentF $\gets$ f(weights)
\State best\_weights $\gets$ weights.copy()
\State

\State max\_neighbours $\gets$ 10 * nº features
\State max\_successes $\gets$ 0.1 * max\_neighbours
\State M $\gets$ nº de enfriamientos a realizar
\State
\State $T_0 \gets$ $\frac{0.3\hspace{0.1cm}*\hspace{0.1cm}currentF}{- \log(0.3)}$
\State $T_F \gets$ $10^{-3}$
\State $\beta \gets \frac{T_0 - T_F}{M * T_0 * T_F}$ 
\State $T_current \gets T_0$
\State
\BState \textbf{\emph{while}}
max\_successes > 0 and $T_{current} > T_F$ and coolings < M
\State neighbours, num\_successes $\gets$ 0

\State \textbf{\emph{while}} num\_successes < max\_successes and negihbours < max\_neighbours

\State \hspace{0.75cm}w $\gets$ weights.copy()
\State \hspace{0.75cm}index $\gets$ componente aleatoria
\State \hspace{0.75cm}w $\gets$ mute(w, index)
\State \hspace{0.75cm}newF $\gets$ f(w)

\State \hspace{0.75cm}$\Delta f$ $\gets$ newF - currentF

\State
\State \hspace{0.75cm}\textbf{if} diff > 0 or U(0,1) $\leq e^{\frac{\Delta f}{T_{current}}}$  \textbf{then}

\State \hspace{1.25cm}currentF $\gets$ newF
\State \hspace{1.25cm}num\_successes $\gets$ +1
\State \hspace{1.25cm}weights $\gets$ w.copy()
\State \hspace{1.25cm}\textbf{if} currentF > bestF \textbf{then} 

\State \hspace{1.75cm}best\_weights $\gets$ weights
\State \hspace{1.75cm}bestF $\gets$ currentF

\State \hspace{1.25cm}neighbours $\gets$ +1

\State \textbf{\emph{endloop}}

\State $T_{current} = \frac{T_{current}}{1+\beta * T_{current}}$
\State coolings $\gets$ +1

\BState \emph{endloop}
\State Entrenamos el modelo, predecimos y calculamos las tasas
\State \textbf{return} tasa clasificación, tasa reducción
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Búsqueda Local Iterativa}

La ILS está basada en la aplicación repetida de la Búsqueda Local a una solución inicial que se obtiene por mutación de un óptimo local previamente encontrado. Siguiendo el pseudocódigo de los seminarios y teoría, se ha implementado una versión concisa aprovechando la búsqueda local implementada en las sesiones anteriores.
\vspace{0.2cm}

\begin{algorithm}
\caption{ILS}\label{euclid}
\begin{algorithmic}[1]
\Procedure{ILS}{data,classes, trainIndex, testIndex}
\State weights $\gets$ generamos la solución inicial
\State bestF, currentF $\gets$ f(weights)
\State best\_weigths $\gets$ weights.copy()
\State mutations $\gets$ range(0.1 * nº features)
\State
\BState \textbf{\emph{for i in}}
(range(15)):
\State \hspace{0.5cm}\emph{for} i in mutations

\State \hspace{1cm}index $\gets$ índice característica aleatoria

\State \hspace{1cm}weigths $\gets$ mute(weights, index, 0.4)
\State \hspace{0.5cm}\emph{endloop}
\State \hspace{0.5cm}weights $\gets$ aplicamos búsqueda local
\State \hspace{0.5cm}currentF $\gets$ f(weights)

\State \hspace{0.5cm}\textbf{if} currentF > bestF  \textbf{then}
\State \hspace{1cm}bestF $\gets$ currentF
\State \hspace{1cm}best\_weights $\gets$ weights
\State
\State \hspace{0.5cm}weights $\gets$ best\_weights.copy()
\BState \emph{endloop}
\State Entrenamos el modelo, predecimos y calculamos las tasas
\State \textbf{return} tasa clasificación, tasa reducción
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Híbrido ILS - ES}

\pagebreak
\section{Procedimiento considerado para desarrollar la práctica}
Para desarrollar la práctica, he usado \textbf{Python} como lenguaje de programación, sin usar ningún framework de metaheurísticas.

Para poder ejecutar el código , hace falta tener instalado \emph{Numpy, Scipy} y \emph{Sklearn}. Este último es muy útil para la realización de prácticas de este estilo pues trae implementaciones de muchas funcionalidades básicas para \emph{Machine learning}.

El fichero que hay que ejecutar dentro de la carpeta es el fichero \emph{\textbf{main.py}}. Para ello, basta con escribir en la terminal:
\[
\textit{python main.py}
\]
Tras la ejecución, comenzará a ejecutar los algoritmos sobre los 3 ficheros de datos que tenemos, que se explicarán más adelante.

La lista de archivos que contiene la práctica son los siguientes:
\begin{itemize}
	\item \textbf{main.py}: fichero principal a ejecutar para la ejecución de nuestro programa.	
	\item \textbf{algorithms.py}: fichero en el que se encuentran los algoritmos de la P1, y algunas funciones auxiliares programadas para la práctica.
\item \textbf{P3\_algorithms.py}: fichero con los algoritmos implementados en esta práctica. 
	\item \textbf{graficas.ipynb}: notebook mediante el cual he generado las gráficas.  
	\item \textbf{Datasets}: carpeta en la que se encuentran los datasets almacenados.
	\item \textbf{Archivos\_CSV}: carpeta con los archivos CSV necesarios para agilizar el proceso de creación de las gráficas.
	
\end{itemize}
\newpage
\section{Experimentos y análisis de resultados}

\subsection{Resultados de cada algoritmo}

\textbf{Algoritmo 1-NN}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.85 & 0.00 & 0.42 & 0.00283 & 	0.72 & 0.00 & 0.36 & 0.00099 & 		0.97 & 0.00 & 0.49 & 0.00309 \\ 
1&	 0.77 & 0.00 & 0.39 & 0.00276 & 	0.82 & 0.00 & 0.41 & 0.00100 & 		0.90 & 0.00 & 0.45 & 0.00323 \\ 
2&	 0.83 & 0.00 & 0.41 & 0.00279 & 	0.95 & 0.00 & 0.47 & 0.00100 & 		0.71 & 0.00 & 0.36 & 0.00312 \\ 
3&	 0.91 & 0.00 & 0.46 & 0.00278 & 	0.74 & 0.00 & 0.37 & 0.00100 & 		0.77 & 0.00 & 0.39 & 0.00317 \\ 
4&	 0.86 & 0.00 & 0.43 & 0.00285 & 	0.67 & 0.00 & 0.33 & 0.00101 & 		0.99 & 0.00 & 0.49 & 0.00307 \\ 
\hline
Media&	 0.84 & 0.00 & 0.42 & 0.00280	& 0.78 & 0.00 & 0.39 & 0.00100	& 0.87 & 0.00 & 0.43 & 0.00314
 \\ Std&	 0.05 & 0.00 & 0.02 & 0.00003	& 0.10 & 0.00 & 0.05 & 0.00001	&	0.11 & 0.00 & 0.05 & 0.00006
 \\ 
\end{tabular}
\end{table}


\textbf{Algoritmo RELIEF}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.86 & 0.03 & 0.44 & 0.01278 & 	0.72 & 0.05 & 0.38 & 0.00627 & 		0.97 & 0.11 & 0.54 & 0.01284 \\ 
1&	 0.79 & 0.03 & 0.41 & 0.01250 & 	0.82 & 0.00 & 0.41 & 0.00475 & 		0.91 & 0.00 & 0.46 & 0.01303 \\ 
2&	 0.80 & 0.09 & 0.44 & 0.01248 & 	0.95 & 0.05 & 0.50 & 0.00474 & 		0.74 & 0.00 & 0.37 & 0.01291 \\ 
3&	 0.91 & 0.03 & 0.47 & 0.01277 & 	0.69 & 0.05 & 0.37 & 0.00474 & 		0.74 & 0.00 & 0.37 & 0.01297 \\ 
4&	 0.86 & 0.03 & 0.44 & 0.01278 & 	0.67 & 0.00 & 0.33 & 0.00476 & 		0.99 & 0.09 & 0.54 & 0.01274 \\ 
\hline
Media&	 0.84 & 0.04 & 0.44 & 0.01266	& 0.77 & 0.03 & 0.40 & 0.00505		&0.87 & 0.04 & 0.46 & 0.01290
 \\ Std&	 0.05 & 0.02 & 0.02 & 0.00014	&0.10 & 0.02 & 0.06 & 0.00061	&	0.11 & 0.05 & 0.08 & 0.00010
 \\

\end{tabular}
\end{table}

\textbf{Algoritmo de búsqueda local}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline

0&	 0.89 & 0.79 & 0.84 & 2.47617 & 	0.77 & 0.68 & 0.73 & 0.54792 & 		0.97 & 0.84 & 0.91 & 3.52364 \\ 
1&	 0.87 & 0.85 & 0.86 & 2.03225 & 	0.85 & 1.00 & 0.92 & 1.16207 & 		0.89 & 0.86 & 0.87 & 4.97461 \\ 
2&	 0.86 & 0.76 & 0.81 & 1.77548 & 	0.87 & 0.82 & 0.84 & 0.81054 & 		0.74 & 0.91 & 0.83 & 3.02352 \\ 
3&	 0.97 & 0.97 & 0.97 & 2.93292 & 	0.74 & 1.00 & 0.87 & 1.17324 & 		0.80 & 0.82 & 0.81 & 2.94800 \\ 
4&	 0.86 & 0.88 & 0.87 & 1.89319 & 	0.62 & 0.91 & 0.76 & 0.53875 & 		0.99 & 0.68 & 0.83 & 3.57642 \\
\hline 
Media&	 0.89 & 0.85 & 0.87 & 2.22200	&0.77 & 0.88 & 0.83 & 0.84650	&	0.88 & 0.82 & 0.85 & 3.60924
 \\ Std&	 0.04 & 0.07 & 0.05 & 0.42744	&0.09 & 0.12 & 0.07 & 0.27982	&	0.09 & 0.08 & 0.04 & 0.72841
 \\  

\end{tabular}
\end{table}
\pagebreak
\textbf{BMB}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.90 & 0.59 & 0.75 & 17.78729 & 	0.94 & 0.68 & 0.81 & 15.10886 & 		0.80 & 0.61 & 0.71 & 20.81741 \\ 
1&	 0.91 & 0.65 & 0.78 & 18.16027 & 	0.91 & 0.68 & 0.80 & 12.36116 & 		0.86 & 0.55 & 0.70 & 20.59248 \\ 
2&	 0.87 & 0.65 & 0.76 & 17.92551 & 	0.92 & 0.73 & 0.83 & 13.88676 & 		0.90 & 0.59 & 0.75 & 21.65683 \\ 
3&	 0.90 & 0.59 & 0.74 & 18.16812 & 	0.94 & 0.73 & 0.83 & 12.77313 & 		0.85 & 0.61 & 0.73 & 18.05538 \\ 
4&	 0.87 & 0.62 & 0.74 & 19.33893 & 	0.94 & 0.73 & 0.83 & 13.36502 & 		0.85 & 0.55 & 0.70 & 22.23772 \\
\hline 
Media&	 0.89 & 0.62 & 0.75 & 18.27602	& 0.93 & 0.71 & 0.82 & 13.49899	&	0.85 & 0.58 & 0.72 & 20.67197
 \\ Std&	 0.02 & 0.03 & 0.01 & 0.55073	&0.01 & 0.02 & 0.01 & 0.95724	&	0.03 & 0.03 & 0.02 & 1.43496
 \\ 
\end{tabular}
\end{table}

\textbf{ES}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.92 & 0.68 & 0.80 & 17.82474 & 	0.97 & 0.73 & 0.85 & 11.80979 & 		0.80 & 0.50 & 0.65 & 21.29503\\
1&	 0.92 & 0.59 & 0.75 & 18.02119 & 	0.96 & 0.77 & 0.86 & 11.81669 & 		0.87 & 0.66 & 0.76 & 21.21374\\
2&	 0.93 & 0.71 & 0.82 & 17.84429 & 	0.95 & 0.77 & 0.86 & 13.23995 & 		0.94 & 0.68 & 0.81 & 23.44971\\
3&	 0.89 & 0.74 & 0.81 & 18.00936 & 	0.98 & 0.86 & 0.92 & 13.59249 & 		0.90 & 0.64 & 0.77 & 20.47733\\
4&	 0.90 & 0.62 & 0.76 & 17.63211 & 	0.99 & 0.64 & 0.81 & 13.07948 & 		0.84 & 0.73 & 0.78 & 21.82140\\
\hline
Media&	 0.91 & 0.66 & 0.79 & 17.86634&	0.97 & 0.75 & 0.86 & 12.70768&		0.87 & 0.64 & 0.76 & 21.65144
\\Std&	 0.01 & 0.05 & 0.03 & 0.14249	&0.02 & 0.07 & 0.03 & 0.74893	&	0.05 & 0.08 & 0.06 & 0.99597
\\
\end{tabular}
\end{table}

\textbf{ILS}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.93 & 0.59 & 0.76 & 20.86034 & 	0.96 & 0.64 & 0.80 & 17.22413 & 		0.84 & 0.55 & 0.69 & 26.56079 \\ 
1&	 0.90 & 0.65 & 0.77 & 20.88104 & 	0.90 & 0.68 & 0.79 & 17.03955 & 		0.86 & 0.61 & 0.74 & 24.52509 \\ 
2&	 0.91 & 0.62 & 0.76 & 20.65761 & 	0.87 & 0.73 & 0.80 & 17.19522 & 		0.94 & 0.57 & 0.75 & 25.26890 \\ 
3&	 0.88 & 0.62 & 0.75 & 20.91860 & 	0.93 & 0.73 & 0.83 & 15.30046 & 		0.89 & 0.55 & 0.72 & 22.21060 \\ 
4&	 0.89 & 0.68 & 0.78 & 20.43069 & 	0.95 & 0.73 & 0.84 & 16.04657 & 		0.77 & 0.66 & 0.71 & 23.19795 \\
\hline 
Media&	 0.90 & 0.63 & 0.76 & 20.74966	&0.92 & 0.70 & 0.81 & 16.56119	&	0.86 & 0.59 & 0.72 & 24.35267
 \\ Std&	 0.02 & 0.03 & 0.01 & 0.18345	&0.03 & 0.04 & 0.02 & 0.76480	&	0.06 & 0.04 & 0.02 & 1.52753
 \\ 
\end{tabular}
\end{table}

\pagebreak
\textbf{ILS - ES}

\begin{table}[ht!]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
0&	 0.93 & 0.94 & 0.93 & 20.62278 & 	0.95 & 0.91 & 0.93 & 17.61132 & 		0.84 & 0.73 & 0.78 & 24.29962 \\ 
1&	 0.94 & 0.91 & 0.92 & 20.76749 & 	0.99 & 1.00 & 0.99 & 14.09746 & 		0.91 & 0.80 & 0.85 & 22.94477 \\ 
2&	 0.94 & 0.85 & 0.90 & 20.79059 & 	0.98 & 0.95 & 0.97 & 15.76394 & 		0.94 & 0.86 & 0.90 & 26.37363 \\ 
3&	 0.91 & 0.85 & 0.88 & 20.64561 & 	0.99 & 0.91 & 0.95 & 16.24410 & 		0.90 & 0.80 & 0.85 & 21.78090 \\ 
4&	 0.90 & 0.85 & 0.87 & 20.28353 & 	0.97 & 0.91 & 0.94 & 15.62938 & 		0.86 & 0.75 & 0.81 & 21.34640 \\ 
\hline
Media&	 0.92 & 0.88 & 0.90 & 20.62200&	0.98 & 0.94 & 0.96 & 15.86924	&	0.89 & 0.79 & 0.84 & 23.34907
 \\ Std&	 0.02 & 0.04 & 0.02 & 0.18150	&0.01 & 0.04 & 0.02 & 1.12988	&	0.04 & 0.05 & 0.04 & 1.82673
 \\ 
\end{tabular}
\end{table}

\subsection{Resumen Global - Tabla de Medias}

\begin{table}[h]
\begin{tabular}{ccccc|cccc|cccc}
\centering
 & \multicolumn{4}{c}{\textit{Ionosphere}} & \multicolumn{4}{c}{\textit{Parkinson}} & \multicolumn{4}{c}{\textit{Spectf-Heart}} \\ \hline
\textbf{Alg} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
1-NN &	0.84 & 0.00 & 0.42 & 0.00280	& 0.78 & 0.00 & 0.39 & 0.00100	& 0.87 & 0.00 & 0.43 & 0.00314 \\
RELIEF &  0.84 & 0.04 & 0.44 & 0.02056 & 	0.77 & 0.03 & 0.40 & 0.00487 & 0.87 & 0.04 & 0.46 & 0.01494\\
BL & 0.89 & 0.85 & 0.87 & 2.22200	&0.77 & 0.88 & 0.83 & 0.84650	&	0.88 & 0.82 & 0.85 & 3.60924\\
AGG-BLX & 0.89 & 0.62 & 0.75 & 18.27602	& 0.93 & 0.71 & 0.82 & 13.49899	&	0.85 & 0.58 & 0.72 & 20.67197 \\
AGG-AC & 0.91 & 0.66 & 0.79 & 17.86634&	0.97 & 0.75 & 0.86 & 12.70768&		0.87 & 0.64 & 0.76 & 21.65144 \\ 
AGE-BLX & 0.90 & 0.63 & 0.76 & 20.74966	&0.92 & 0.70 & 0.81 & 16.56119	&	0.86 & 0.59 & 0.72 & 24.35267 \\
AGE-AC & 0.92 & 0.88 & 0.90 & 20.62200&	0.98 & 0.94 & 0.96 & 15.86924	&	0.89 & 0.79 & 0.84 & 23.34907 \\
AM1 & 0.92 & 0.84 & 0.88 & 19.39848	&0.95 & 0.90 & 0.92 & 14.27740	&	0.88 & 0.80 & 0.84 & 21.06626 \\
AM2 & 0.90 & 0.71 & 0.80 & 19.68916	&0.94 & 0.72 & 0.83 & 12.72460	&	0.86 & 0.68 & 0.77 & 20.27860 \\
AM3 & 0.91 & 0.77 & 0.84 & 22.03035	&0.95 & 0.83 & 0.89 & 13.35908	&	0.88 & 0.73 & 0.80 & 20.43009 \\
\end{tabular}
\end{table}

\subsection{Análisis de los resultados}

En esta sección realizaremos un extenso análisis a cerca de los resultados obtenidos. Para ello analizaremos cada una de las tasas. Empezamos en primer lugar por el tiempo de ejecución. 

\begin{tcolorbox}
\textbf{Primero, mencionar que para un mejor análisis visual recomiendo acudir al archivo .ipynb. Puesto que Plotly es una librería que permite la realización de gráficos interactivos, se pueden estudiar mucho mejor las comparativas. A continuación incluyo mis conclusiones tras examinar las distintas gráficas.}
\end{tcolorbox}

\subsubsection{Tiempo de ejecución}
\subsubsection{Tasa de Clasificación}
\subsubsection{Tasa de Reducción}
\subsubsection{Función Objetivo}
\subsubsection{Análisis de convergencia}

  

\end{document}